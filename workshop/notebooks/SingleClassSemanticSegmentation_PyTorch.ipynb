{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "**Please use Colab Runtime Version 25.07 --> Python 3.11.13**\n",
    "\n",
    "We will soon release czmodel = 6.0.0,which will also support newer python versions.\n",
    "\n",
    "## Disclaimer\n",
    "\n",
    "This content of this repository is free to use for everybody. Carl Zeiss Microscopy GmbH's ZEN software undertakes no warranty concerning the use of those scripts, image analysis settings and ZEN experiments. Use them on your own risk.\n",
    "\n",
    "**By using any of those examples you agree to this disclaimer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHZX_QBJAsxR",
    "outputId": "6710984f-7d19-4ec8-d52b-69cf0c57c463",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    \n",
    "    # Install czmodel and dependencies\n",
    "    ! pip install --upgrade pip\n",
    "    ! pip install \"torch==2.5.1\"\n",
    "    ! pip install \"torchvision==0.20.1\"\n",
    "    ! pip install \"czmodel[pytorch]==5.1.0\"\n",
    "    ! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5_xbvxuAsxV",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# This can be used to switch on/off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The library [czmodel](https://pypi.org/project/czmodel/) provides a base package and extras for export functionalities that require specific dependencies -:\n",
    "\n",
    "- ```pip install czmodel``` - This would only install base dependencies, no Pytorch-specific packages.\n",
    "- ```pip install czmodel[pytorch]``` - This would install base and Pytorch-specific packages.\n",
    "\n",
    "**This is an example notebook for segmentation task with czmodel[pytorch].**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQ3IRLvTAsxW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple PyTorch model for segmentation (to detect cell nuclei)\n",
    "This notebook shows the entire workflow of training an ANN with [PyTorch](https://pytorch.org/) and exporting the trained model to the [CZANN format](https://pypi.org/project/czmodel/) to be ready for used within ZEN & arivis software or your own python code.\n",
    "\n",
    "* The trained model is rather simple (for demo purposes) and trained on a small test dataset.\n",
    "* **Therefore, this notebook is meant to be understood as a guide for exporting trained models.**\n",
    "* **The notebook does not provide instructions how train a model correctly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cu9fv3kAAsxX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wt7s_-SeAsxX",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Required imports to train a simple PyTorch model for segmentation and package it as CZANN.\n",
    "# The CZANN can then be imported in ZEN and used for segmentation and image analysis workflows.\n",
    "\n",
    "# General imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Function provided by the PyPI package called czmodel (by ZEISS)\n",
    "from czmodel import ModelMetadata, ModelType, LegacyModelMetadata\n",
    "from czmodel.pytorch import ModelSpec, LegacyModelSpec, DefaultConverter, LegacyConverter\n",
    "\n",
    "import logging\n",
    "from typing import List\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "# This can be used to switch on/off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "logging.getLogger(\"pytorch\").setLevel(logging.ERROR)\n",
    "print(torch.__version__)\n",
    "\n",
    "# Use GPU if available otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7UMTdi3AsxY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Pipeline\n",
    "This section describes a simple training procedure that creates a trained PyTorch model.\n",
    "\n",
    "* Therefore, it only represents the custom training procedure\n",
    "* Such procedure will vary from case to case and will contain more sophisticated ways to generate an optimized PyTorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5M5XQV24AsxZ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define parameters for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68g3o9M1Asxa",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Folder containing the input images\n",
    "IMAGES_FOLDER = 'nucleus_data/images/'\n",
    "\n",
    "# Folder containing the ground truth segmentation masks\n",
    "# Mask images have one channel with a unique value of each class (0=background and 1=nucleus)\n",
    "MASKS_FOLDER = 'nucleus_data/labels/'\n",
    "\n",
    "# Path to the data on GitHub\n",
    "GITHUB_TRAINING_DATA_PATH = \"https://raw.githubusercontent.com/sebi06/ZEN_Python_CZI_Smart_Microscopy_Workshop/main/workshop/notebooks/nucleus_data.zip\"\n",
    "GITHUB_MODEL_CONVERSION_SPEC_PATH = \"https://raw.githubusercontent.com/sebi06/ZEN_Python_CZI_Smart_Microscopy_Workshop/main/workshop/notebooks/models/model_conversion_spec_pytorch.json\"\n",
    "\n",
    "# Define the number of input color channels\n",
    "# This means that the inputs are grayscale with one channel only\n",
    "CHANNELS = 1 \n",
    "\n",
    "# The size of image crops to train the model with\n",
    "CROP_SIZE = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data if it's not available on disk\n",
    "If this notebook is run e.g. as a colab notebook, it does not have access to the data folder on gitub via disk access. \n",
    "In that case we need to download the data from github first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Download training data\n",
    "if not (os.path.isdir(IMAGES_FOLDER) and os.path.isdir(MASKS_FOLDER)):\n",
    "    compressed_data = './nucleus_data.zip'\n",
    "    if not os.path.isfile(compressed_data):\n",
    "        import io\n",
    "        response = requests.get(GITHUB_TRAINING_DATA_PATH, stream=True)\n",
    "        compressed_data = io.BytesIO(response.content)\n",
    "\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(compressed_data, 'r') as zip_accessor:\n",
    "        zip_accessor.extractall('./')\n",
    "\n",
    "# Download model conversion spec\n",
    "if not os.path.isfile('./models/model_conversion_spec_pytorch.json'):\n",
    "\n",
    "    if not os.path.isdir(\"./models\"):\n",
    "        os.mkdir(\"models\")\n",
    "    \n",
    "    response = requests.get(GITHUB_MODEL_CONVERSION_SPEC_PATH, stream=True)\n",
    "    with open('./models/model_conversion_spec_pytorch.json', 'wb') as handle:\n",
    "        handle.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBn9VvQqAsxa",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define a PyTorch dataset to pre-process the images\n",
    "Since the dataset contains very large images we need to train on smaller crops in order to not exhaust the GPU memory.\n",
    "\n",
    "In order to use the model later inside Napari, we read the test images (PNGs) as grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from skimage.io import imread\n",
    "\n",
    "# Dataset class\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self, images_folder:str, masks_folder:str, transforms: torchvision.transforms=None):\n",
    "        self.sample_images = sorted([os.path.join(images_folder, f) for f in os.listdir(images_folder) \n",
    "                        if os.path.isfile(os.path.join(images_folder, f))])\n",
    "        self.sample_masks = sorted([os.path.join(masks_folder, f) for f in os.listdir(masks_folder) \n",
    "                       if os.path.isfile(os.path.join(masks_folder, f))])\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image = self.sample_images[idx]\n",
    "        mask = self.sample_masks[idx]\n",
    "\n",
    "        # this makes sure that the PNGs are read a grayscale\n",
    "        img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "        msk = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "            msk = self.transforms(msk)\n",
    "\n",
    "        # Randomly crop the image\n",
    "        i, j, h, w = torchvision.transforms.RandomCrop.get_params(img, output_size=(CROP_SIZE, CROP_SIZE))\n",
    "        img = TF.crop(img, i, j, h, w)\n",
    "        msk = TF.crop(msk, i, j, h, w)\n",
    "\n",
    "        # Transform mask to one-hot encoding\n",
    "        msk = msk/msk.max()\n",
    "        msk = torch.nn.functional.one_hot(msk.long(), num_classes=2).squeeze().permute(2, 0, 1)\n",
    "\n",
    "        return img.to(torch.float32), msk.to(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "from torchvision import transforms\n",
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "sample_dataset = SampleDataset(images_folder=IMAGES_FOLDER,\n",
    "                               masks_folder=MASKS_FOLDER,\n",
    "                               transforms=transforms)\n",
    "\n",
    "sample_dataloader = DataLoader(dataset=sample_dataset,\n",
    "                                      batch_size=2,\n",
    "                                      shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data is loaded correctly\n",
    "print(\"Data Verification:\")\n",
    "print(f\"Total samples in dataset: {len(sample_dataset)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for img, mask in sample_dataset:\n",
    "    print(f\"Image shape: {img.shape}, min: {img.min():.3f}, max: {img.max():.3f}\")\n",
    "    print(f\"Mask shape: {mask.shape}, unique values: {torch.unique(mask)}\")\n",
    "    print(f\"Mask class distribution:\")\n",
    "    print(f\"  - Background (class 0): {(mask[0] == 1).sum().item()} pixels\")\n",
    "    print(f\"  - Nucleus (class 1): {(mask[1] == 1).sum().item()} pixels\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcumQa0yAsxb",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define a simple model\n",
    "This part defines a PyTorch model with two convolutional layers and softmax activation at the output node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jW6eR6s8Asxc",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define simple PyTorch model with two convolutional layers and softmax activation at the output node\n",
    "class SampleModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SampleModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.Conv2d(16, 2, 1, padding=0),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SampleModel().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpBMYhGzAsxc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training the model with the loaded data\n",
    "This part fits the model to the loaded data. In this test example we do not care about an actual evaluation of the model using validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSSU22TtAsxd",
    "outputId": "7854dc6c-3fc4-426c-a5c3-6328b0c5f4c8",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the number of training epochs\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2, eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index: int):\n",
    "    \"\"\"Train the model for one epoch with gradient clipping.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(sample_dataloader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping (prevents exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(sample_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define plotting function\n",
    "def plot_history(loss: List):\n",
    "    plt.plot(loss, \"*-\")\n",
    "    plt.title('training loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the simple training and show the loss with better monitoring\n",
    "training_loss = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Number of training samples: {len(sample_dataset)}\")\n",
    "print(f\"Batch size: {sample_dataloader.batch_size}\")\n",
    "print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss = train_one_epoch(epoch)\n",
    "    training_loss.append(loss)\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f'Epoch {epoch+1:3d}/{NUM_EPOCHS}, Loss: {loss:.6f}')\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Final loss: {training_loss[-1]:.6f}\")\n",
    "print(f\"Initial loss: {training_loss[0]:.6f}\")\n",
    "print(f\"Loss reduction: {training_loss[0] - training_loss[-1]:.6f}\")\n",
    "\n",
    "# Plot the training history\n",
    "plot_history(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(net,\n",
    "                img,\n",
    "                device):\n",
    "    net.eval()\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device=device, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = net(img).cpu()\n",
    "\n",
    "        mask = output.argmax(dim=1)\n",
    "        \n",
    "        pred = mask[0].long().squeeze().numpy()\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.to(device=device)\n",
    "\n",
    "# get an image and a mask from the dataset\n",
    "for img, mask in sample_dataset:\n",
    "    \n",
    "    # run the prediction\n",
    "    pred = predict_img(net=model,\n",
    "                       img=img,\n",
    "                       device=device)\n",
    "    \n",
    "    # show some results\n",
    "    img_scaled = img.numpy()[0] / img.numpy().max()\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "    fig.set_figwidth(16)\n",
    "\n",
    "    # show images and set titles \n",
    "    ax1.imshow(img_scaled)\n",
    "    ax2.imshow(pred, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    ax3.imshow(mask[0,...])\n",
    "\n",
    "    ax1.set_title('Image')\n",
    "    ax2.set_title('Prediction')\n",
    "    ax3.set_title('Label Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to cpu and save it to disk\n",
    "if not os.path.isdir(\"./models\"):\n",
    "    os.mkdir(\"models\")\n",
    "PATH = './models/saved_torch_segmentation_model.pt'\n",
    "model.cpu()\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYV2aL42Asxd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create a CZANN/CZMODEL from the trained PyTorch model\n",
    "In this section we export the trained model to the CZANN format using the czmodel library and some additional meta data all possible parameter choices are described in the [ANN model specification](https://pypi.org/project/czmodel/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF8bt-ZGAsxd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define Meta Data\n",
    "We first define the meta data needed to run the model within the Intellesis infrastructure. The `czmodel` package offers a named tuple `ModelMetadata` that allows to either parse as JSON file as described in the [specification document](https://pypi.org/project/czmodel/) or to directly specify the parameters as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create a Model Specification Object\n",
    "The export functions provided by the `czmodel` package expect a `ModelSpec` tuple that features the PyTorch model to be exported, the corresponding model meda data and optionally a license file for the model.\n",
    "\n",
    "Therefore, we wrap our model and the `model_metadata` instance into a `ModelSpec` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfpCKOp2Asxe",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define dimensions - ZEN or Arivis Pro require fully defined spatial dimensions in the meta data of the CZANN model.\n",
    "# The TilingClient uses the input shape in the meta data to infer the tile size to pass an image to the inferencer.\n",
    "# Important: The tile size has to be chosen s.t. inference is possible with the minimum hardware requirements\n",
    "# Optional: Define target spatial dimensions of the model for inference.\n",
    "\n",
    "input_size = 512\n",
    "\n",
    "# Define the model metadata\n",
    "model_metadata = ModelMetadata(\n",
    "    input_shape=[input_size, input_size, 1],\n",
    "    output_shape=[input_size, input_size, 2],\n",
    "    model_type=ModelType.SINGLE_CLASS_SEMANTIC_SEGMENTATION,\n",
    "    classes=[\"Background\", \"Nucleus\"],\n",
    "    model_name=\"Simple_Nuclei_SegmentationModel\",\n",
    "    min_overlap=[64, 64],\n",
    "    scaling=(1.0, 1.0),\n",
    ")\n",
    "model_spec = ModelSpec(\n",
    "    model=model,\n",
    "    model_metadata=model_metadata,\n",
    "    license_file=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMzxQONdAsxe",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perform model export into *.czann / *.czmodel file format\n",
    "\n",
    "The converters from the `czmodel` library offers two functions to perform the actual export. \n",
    "\n",
    "* `convert_from_json_spec` allows to provide a JSON file containing all the information of a ModelSpec object and converts a model in PyTorch model format on disk to a `.czann` / `.czmodel` file that can be loaded with ZEN.\n",
    "* `convert_from_model_spec` expects a `ModelSpec` object, an output path and name and the input shape of the exported model. From this information it creates a `.czann` / `.czmodel` file containing the specified model.\n",
    "\n",
    "Currently, `czmodel` offers two converters:\n",
    "* DefaultConverter: Converts a model to a *.czann file.\n",
    "* LegacyConverter: Converts a model to a *.czmodel file (legacy format).\n",
    "\n",
    "We will only use the `DefaultConverter` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hfDNDwOAsxe",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "DefaultConverter().convert_from_model_spec(\n",
    "    model_spec=model_spec, \n",
    "    output_path='./models', \n",
    "    output_name='simple_pytorch_nuclei_segmodel_pytorch',\n",
    "    input_shape=(1, input_size, input_size),\n",
    ")\n",
    "\n",
    "# In the example above there will be a \"./simple_nuclei_segmodel_pytorch.czann\" file saved on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRA1rIS-Asxf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remarks\n",
    "The generated .czann and .czmodel files can be directly loaded into ZEN Intellesis to perform segmentation tasks with the trained model.\n",
    "If there is already a trained model in PyTorch model format present on disk, it can also be converted by providing the path to the saved model directory instead of a PyTorch `Module` object. The `czmodel` library will implicitly load the model from the provided path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kF_QR6BAsxi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `czmodel` library also provides a `convert_from_json_spec` function that accepts a JSON file with the above mentioned meta data behind the key `ModelMetadata` which will implicitly be deserialized into a `ModelMetadata` object, the model path and optionally a license file:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"ModelMetadata\": {\n",
    "        \"Type\": \"SingleClassSemanticSegmentation\",\n",
    "        \"Classes\": [\"Background\", \"Nucleus\"],\n",
    "        \"InputShape\": [512, 512, 1],\n",
    "        \"OutputShape\": [512, 512, 2],\n",
    "        \"ModelName\": \"Nuclei Segmentation Model From JSON\",\n",
    "        \"MinOverlap\": [64, 64],\n",
    "        \"Scaling\": [1.0, 1.0]\n",
    "    },\n",
    "    \"ModelPath\": \"./models/saved_torch_segmentation_model.pt\",\n",
    "    \"LicenseFile\": null\n",
    "}\n",
    "```\n",
    "\n",
    "This information can be copied to a file e.g. in the current working directory `./models/model_conversion_spec_pytorch.json` that also contains the trained model in SavedModel format e.g. generated by the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPCZdZoYAsxi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This is an additional way to create a CZANN from a saved PyTorch model on disk + JSON file.\n",
    "# The currently recommended way to to create the CZANN directly by using czmodel.convert_from_model_spec\n",
    "# the path to the saved Pytorch model is defined in the JSON shown above\n",
    "\n",
    "DefaultConverter().convert_from_json_spec(\n",
    "    model_spec_path='./models/model_conversion_spec_pytorch.json',\n",
    "    output_path='./models',\n",
    "    output_name = 'simple_nuclei_segmodel_from_json_pytorch',\n",
    "    input_shape=(1, input_size, input_size),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SingleClassSemanticSegmentation_PyTorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "smartmic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
